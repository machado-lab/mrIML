#'mrTidyPerf:  Wrapper to calculate performance metrics (Mathews correlation coefficent, sensitivity
#'and specificity) for each model for each response variable. 
#'
#'Outputs a dataframe of commonly used metric that can be used to compare model performance of classification models.
#'
#'@param yhats #is the list generated by MrTidyPerf
#'@param model1 #the model used to generate the yhats object 
#'@param X  #is a response variable data set (specie, SNPs etc).
#'

mrTidyPerf <- function(yhats, model1, X){
  
  n_response<- length(yhats)
  mod_perf <- NULL
  
  yList <- yhats %>% purrr::map(pluck('yhatT')) #get the training yhats all together
    
    #modelperf <- map(seq(1,n_response), function(i){
  for( i in 1:n_response) {
      
    yd <- as.data.frame(yList[i])
    mathews <-  mcc(yd,class, .pred_class)
    mathews <- mathews$.estimate
    sen <- sens(yd,class, .pred_class)
    sen <- sen$.estimate
    spe <- spec(yd,class, .pred_class)
    spe <- spe$.estimate
    #rocAUC <- roc_auc(yhatT,class, .pred_class) #not working for some reason. Wants pred_class as numeric?

#add some identifiers
  mod_name <- class(model1)[1]
  
  #model1_perf <- c(mod_name, mathews, sen, spe)
  sp <- names(X[i])

#save all the metrics
  mod_perf[[i]] <- c( sp, mod_name, mathews, sen, spe)

    }
  mod1_perf<- do.call(rbind, mod_perf)
  mod1_perf <- as.data.frame( mod1_perf)
  colnames(mod1_perf) <- c('Species', 'Model_name', 'mcc', 'sensitivity', 'specificity')
  
  return ( mod1_perf)
    
    #set_names(c('Model_name', 'mcc', 'sensitivity', 'specificity'))
    #bind_rows( modelperf)
   
  
   #row.names(model1_perf1) <- c('Model_name', 'mcc', 'sensitivity', 'specificity')
}
